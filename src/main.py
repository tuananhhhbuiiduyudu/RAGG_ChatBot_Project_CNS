import streamlit as st
from dotenv import load_dotenv
from crawl import craw_web, save_data_local
from seed_data import seed_faiss, seed_faiss_live
from agent import get_retriever, get_llm_and_agent
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory


def setup_page():
    st.set_page_config(
        page_title="AI Assistant",
        page_icon="üí¨",
        layout="wide"
    )


def initialize_app():
    load_dotenv()
    setup_page()


def setup_sidebar():
    with st.sidebar:
        st.title("‚öôÔ∏è C·∫•u h√¨nh")

        st.header("üî§ Embeddings Model")
        st.write("S·ª≠ d·ª•ng HuggingFace Embeddings (all-MiniLM-L6-v2)")

        st.header("üìö Ngu·ªìn d·ªØ li·ªáu")
        data_source = st.radio(
            "Ch·ªçn ngu·ªìn d·ªØ li·ªáu:",
            ["File Local", "URL tr·ª±c ti·∫øp"],
            help="Ch·ªçn c√°ch t·∫£i d·ªØ li·ªáu v√†o vectorstore"
        )

        if data_source == "File Local":
            handle_local_file()
        else:
            handle_url_input()

        st.header("üîç Collection ƒë·ªÉ truy v·∫•n")
        collection_to_query = st.text_input(
            "Nh·∫≠p t√™n collection c·∫ßn truy v·∫•n:",
            "data_test",
            help="Nh·∫≠p t√™n collection b·∫°n mu·ªën s·ª≠ d·ª•ng ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin"
        )

        st.header("ü§ñ Model AI")
        st.write("S·ª≠ d·ª•ng Grok (Llama3-8b-8192)")
        model_choice = "groq"

        return model_choice, collection_to_query


def handle_local_file():
    collection_name = st.text_input("T√™n collection trong FAISS:", "data_test")
    filename = st.text_input("T√™n file JSON:", "stack.json")
    directory = st.text_input("Th∆∞ m·ª•c ch·ª©a file:", "data")

    if st.button("T·∫£i d·ªØ li·ªáu t·ª´ file"):
        if not collection_name or not filename or not directory:
            st.error("Vui l√≤ng nh·∫≠p ƒë·∫ßy ƒë·ªß th√¥ng tin!")
            return

        with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu..."):
            try:
                vectorstore = seed_faiss(
                    collection_name=collection_name,
                    filename=filename,
                    directory=directory,
                    save_path="./vectorstores"
                )
                st.success(f"ƒê√£ t·∫£i d·ªØ li·ªáu th√†nh c√¥ng v√†o collection '{collection_name}'!")
                st.info(f"S·ªë documents: {len(vectorstore.docstore._dict)}")
            except Exception as e:
                st.error(f"L·ªói khi t·∫£i d·ªØ li·ªáu: {str(e)}")


def handle_url_input():
    collection_name = st.text_input("T√™n collection trong FAISS:", "data_test_live")
    url = st.text_input("Nh·∫≠p URL:", "https://www.stack-ai.com/docs")
    doc_name = st.text_input("T√™n t√†i li·ªáu:", "stack-ai")

    if st.button("Crawl d·ªØ li·ªáu"):
        if not collection_name or not url or not doc_name:
            st.error("Vui l√≤ng nh·∫≠p ƒë·∫ßy ƒë·ªß th√¥ng tin!")
            return

        with st.spinner("ƒêang crawl d·ªØ li·ªáu..."):
            try:
                vectorstore = seed_faiss_live(
                    URL=url,
                    collection_name=collection_name,
                    doc_name=doc_name,
                    save_path="./vectorstores"
                )
                st.success(f"ƒê√£ crawl d·ªØ li·ªáu th√†nh c√¥ng v√†o collection '{collection_name}'!")
                st.info(f"S·ªë documents: {len(vectorstore.docstore._dict)}")
            except Exception as e:
                st.error(f"L·ªói khi crawl d·ªØ li·ªáu: {str(e)}")


def setup_chat_interface(model_choice):
    st.title("üí¨ AI Assistant")
    st.caption("üöÄ Tr·ª£ l√Ω AI ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi LangChain v√† xAI Grok")

    msgs = StreamlitChatMessageHistory(key="langchain_messages")

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"}
        ]
        msgs.add_ai_message("T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?")

    for msg in st.session_state.messages:
        role = "assistant" if msg["role"] == "assistant" else "human"
        st.chat_message(role).write(msg["content"])

    return msgs


def handle_user_input(msgs, agent_executor):
    if prompt := st.chat_input("H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ Stack AI!"):
        st.session_state.messages.append({"role": "human", "content": prompt})
        st.chat_message("human").write(prompt)
        msgs.add_user_message(prompt)

        with st.chat_message("assistant"):
            st_callback = StreamlitCallbackHandler(st.container())
            chat_history = [
                {"role": msg["role"], "content": msg["content"]}
                for msg in st.session_state.messages[:-1]
            ]
            try:
                response = agent_executor.invoke(
                    {
                        "input": prompt,
                        "chat_history": chat_history
                    },
                    {"callbacks": [st_callback]}
                )
                output = response["output"]
            except Exception as e:
                output = f"L·ªói khi x·ª≠ l√Ω y√™u c·∫ßu: {str(e)}"

            st.session_state.messages.append({"role": "assistant", "content": output})
            msgs.add_ai_message(output)


def main():
    initialize_app()
    model_choice, collection_to_query = setup_sidebar()
    msgs = setup_chat_interface(model_choice)

    try:
        retriever = get_retriever(collection_name=collection_to_query)
        agent_executor = get_llm_and_agent(retriever, model_choice="groq")
    except Exception as e:
        st.error(f"L·ªói khi kh·ªüi t·∫°o agent: {str(e)}. H√£y ch·∫Øc ch·∫Øn r·∫±ng b·∫°n ƒë√£ t·∫£i d·ªØ li·ªáu v√†o collection n√†y.")
        return

    handle_user_input(msgs, agent_executor)


if __name__ == "__main__":
    main()
